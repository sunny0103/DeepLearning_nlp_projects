{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyOzUiBRMpS5UTtL+fY1Py69",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunny0103/DeepLearning_nlp_projects/blob/main/nlg_practice/Wikitext_NLG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VJJW-t2YGQlz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f68c7679-2930-495a-d149-37cedefa8313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir my_data\n",
        "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" > /dev/null\n",
        "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" -o my_data/wiki_small.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs4ML8ZBLuA5",
        "outputId": "f82e30ab-201e-4cc1-9991-7674ac671cf8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘my_data’: File exists\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1323k  100 1323k    0     0  1139k      0  0:00:01  0:00:01 --:--:-- 17.4M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/my_data/wiki_small.txt'"
      ],
      "metadata": {
        "id": "LVqzkgZoPtuk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "import warnings\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "from tokenizers import SentencePieceBPETokenizer\n",
        "from tokenizers.normalizers import BertNormalizer\n",
        "\n",
        "from transformers import (GPT2Config,\n",
        "                          GPT2LMHeadModel,\n",
        "                          DataCollatorForLanguageModeling,\n",
        "                          Trainer,\n",
        "                          TrainingArguments)\n",
        "from transformers.tokenization_utils import PreTrainedTokenizer\n",
        "from transformers.utils import logging\n",
        "\n",
        "from filelock import FileLock\n"
      ],
      "metadata": {
        "id": "vM7ua6u8IZab"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SentencePieceBPETokenizer()"
      ],
      "metadata": {
        "id": "UBN7mvzqQGEC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer._tokenizer.normalizer = BertNormalizer(\n",
        "    clean_text = True,\n",
        "    handle_chinese_chars=False,\n",
        "    lowercase=False\n",
        ")"
      ],
      "metadata": {
        "id": "yVvKaVzoRLng"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train tokenizer"
      ],
      "metadata": {
        "id": "gTfcy0JdSThv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.train(\n",
        "    path,\n",
        "    vocab_size= 10000,\n",
        "    special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"],\n",
        ")"
      ],
      "metadata": {
        "id": "j5LShuM6Rutl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text=\"문장생성 실습을 위한 샘플 텍스트 입니다.\""
      ],
      "metadata": {
        "id": "5_DCfBIZSzya"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.encode(sample_text))\n",
        "print(tokenizer.encode(sample_text).ids)\n",
        "print(tokenizer.encode(sample_text).tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLpUvVjJS94h",
        "outputId": "2e6d6ad9-f150-40b6-83f7-5096478c395f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding(num_tokens=14, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "[1073, 723, 545, 556, 1083, 589, 702, 1553, 105, 3, 930, 2984, 1203, 3234]\n",
            "['▁문', '장', '생', '성', '▁실', '습', '을', '▁위한', '▁', '<unk>', '플', '▁텍스트', '▁입', '니다.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(tokenizer.encode(sample_text).ids, skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0Gv7RkaTcwK",
        "outputId": "b22ab7e3-db2f-4e7f-cd12-2df394484a56"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문장생성 실습을 위한 플 텍스트 입니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save trained tokenizer**"
      ],
      "metadata": {
        "id": "8ipQpuPRSpkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_model(\".\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79PizrHBSSjG",
        "outputId": "a83aa0bf-4fac-4c96-d500-85dbf853b83f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./vocab.json', './merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SentencePieceBPETokenizer.from_file(vocab_filename=\"vocab.json\", merges_filename=\"merges.txt\")"
      ],
      "metadata": {
        "id": "k0OdFUTET-MK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.add_special_tokens([\"<s>\", \"</s>\", \"<unk>\", \"<pad>\"])"
      ],
      "metadata": {
        "id": "XZ3D0dD8UIAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2db09775-e005-4785-b7a0-4e69a9504423"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.bos_token_id = tokenizer.token_to_id(\"<bos>\")\n",
        "tokenizer.eos_token_id = tokenizer.token_to_id(\"<eos>\")\n",
        "tokenizer.unk_token_id = tokenizer.token_to_id(\"<unk>\")\n",
        "tokenizer.pad_token_id = tokenizer.token_to_id(\"<pad>\")"
      ],
      "metadata": {
        "id": "vjmJBiWUHViz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_add_text = \"<s>\" + sample_text +\"</s>\""
      ],
      "metadata": {
        "id": "0cVFo1txIAaz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.encode(sample_add_text))\n",
        "print(tokenizer.encode(sample_add_text).ids)\n",
        "print(tokenizer.encode(sample_add_text).tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swaQleU9IVYm",
        "outputId": "1c0aaa50-bd50-48f5-bdd2-546652a1ee36"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "[0, 1073, 723, 545, 556, 1083, 589, 702, 1553, 105, 3, 930, 2984, 1203, 3234, 2]\n",
            "['<s>', '▁문', '장', '생', '성', '▁실', '습', '을', '▁위한', '▁', '<unk>', '플', '▁텍스트', '▁입', '니다.', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPT2Config(\n",
        "    vocab_size = tokenizer.get_vocab_size(),\n",
        "    bos_token_id = tokenizer.token_to_id(\"<s>\"),\n",
        "    eos_token_id = tokenizer.token_to_id(\"</s>\")\n",
        ")\n",
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MWUhfRBIyh2",
        "outputId": "9f28a76b-72a1-43cf-9571-e8b4ae495f09"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Config {\n",
              "  \"activation_function\": \"gelu_new\",\n",
              "  \"attn_pdrop\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"embd_pdrop\": 0.1,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"layer_norm_epsilon\": 1e-05,\n",
              "  \"model_type\": \"gpt2\",\n",
              "  \"n_embd\": 768,\n",
              "  \"n_head\": 12,\n",
              "  \"n_inner\": null,\n",
              "  \"n_layer\": 12,\n",
              "  \"n_positions\": 1024,\n",
              "  \"reorder_and_upcast_attn\": false,\n",
              "  \"resid_pdrop\": 0.1,\n",
              "  \"scale_attn_by_inverse_layer_idx\": false,\n",
              "  \"scale_attn_weights\": true,\n",
              "  \"summary_activation\": null,\n",
              "  \"summary_first_dropout\": 0.1,\n",
              "  \"summary_proj_to_labels\": true,\n",
              "  \"summary_type\": \"cls_index\",\n",
              "  \"summary_use_proj\": true,\n",
              "  \"transformers_version\": \"4.34.1\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 10000\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel(config)"
      ],
      "metadata": {
        "id": "_MvstHMVKn13"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.num_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sytlqn8dK387",
        "outputId": "fcfeadb2-a2cc-4426-d220-67719dd63cee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93522432"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTDataset(Dataset):\n",
        "  def __init__(self, tokenzier, file_path, block_size):\n",
        "    block_size = block_size - tokenizer.num_special_tokens_to_add(is_pair=False)\n",
        "    text =\"\"\n",
        "    with open (path, encoding=\"utf-8\") as f:\n",
        "      lines = f.readlines()\n",
        "      for line in lines:\n",
        "        line = line.strip()\n",
        "        line = \"<s>\"+line+\"</s>\"\n",
        "        text += line\n",
        "\n",
        "    tokenzied_text = tokenizer.encode(text).ids\n",
        "\n",
        "    self.dataset = []\n",
        "    for i in range(0, len(tokenzied_text)-block_size+1, block_size):\n",
        "      self.dataset.append(tokenzied_text[i:i+block_size])\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      return torch.tensor(self.dataset[index],dtype=torch.long)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.dataset)\n"
      ],
      "metadata": {
        "id": "xA36pSg9REd_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = GPTDataset(\n",
        "    tokenzier = tokenizer,\n",
        "    file_path = path,\n",
        "    block_size = 128\n",
        ")"
      ],
      "metadata": {
        "id": "1eVg06upVMZV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnyjqAf_Va2G",
        "outputId": "b562a5a7-d34a-4b5b-f6df-2fe1fc5dbaaa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   0, 3997, 3546, 8406,  462,    5, 5481, 9526, 1798, 1890, 2297, 1262,\n",
              "        9624, 2679, 1188, 2174,    2,    0, 5709, 5481,  254, 6466,  750, 3426,\n",
              "         872, 1556,  680,  894, 1626, 9222,  586, 3620, 1010, 3303,    2,    0,\n",
              "        6466, 7418, 2305,  402, 2217, 1074,    2,    0, 1013, 1107, 3715,  645,\n",
              "        8576, 1024,  940,   93, 7323,  370,   93,  721, 9293,  705, 1651,  452,\n",
              "        3167, 1032, 1074,    2,    0, 6343, 1262, 3715, 1009, 2932, 1176,  913,\n",
              "        2037, 1171, 3228,  843,   93,  438,  974, 1486, 1017,    3, 1323, 3913,\n",
              "        2095, 1042,    2,    0, 1382, 2068, 2225, 1095,  325,  843, 1823,  506,\n",
              "           5, 1240, 7698,    2,    0, 3896, 6466, 1053, 1077,  686, 2318, 4649,\n",
              "        5205, 5672, 1013, 1759,  116, 2742, 3004,  105,  655, 2283, 9763, 1192,\n",
              "        1796, 2449, 2546, 9936, 6466, 1053, 1037,  533])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "6TuX6VRAVeiq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir = \"gpt2_model_output\",\n",
        "    num_train_epochs=120,\n",
        "    per_device_train_batch_size=64,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=600\n",
        "\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    data_collator= collator,\n",
        "    train_dataset = dataset\n",
        ")"
      ],
      "metadata": {
        "id": "cKN8lT0wWAYj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "tf2Hj5oNXv6d",
        "outputId": "f90431a3-d2de-4493-f088-03830fc5b557"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1800' max='1800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1800/1800 14:42, Epoch 60/60]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>7.187200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>5.903800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>5.030600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>4.377200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>3.914300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>3.649900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1800, training_loss=5.010489298502605, metrics={'train_runtime': 883.3351, 'train_samples_per_second': 128.853, 'train_steps_per_second': 2.038, 'total_flos': 7435064770560000.0, 'train_loss': 5.010489298502605, 'epoch': 60.0})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "X_jNB3oBYy_C"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "vjBtCNl6hYtl"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NNoK4J5hi1m",
        "outputId": "bfcb5c5a-c201-4dd2-838d-f793567af9c9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7dcfb4771330>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor(tokenizer.encode(\"<s> 문장생성 실습을\", add_special_tokens=True).ids).unsqueeze(0).to('cuda')"
      ],
      "metadata": {
        "id": "rlz3jHRBhl2x"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Greedy\n",
        "output_sentences = model.generate(input_ids = input_ids,\n",
        "                                  do_sample = True,\n",
        "                                  max_length=100,\n",
        "                                  num_return_sequences=3\n",
        "                                  )\n",
        "\n",
        "for generated_sentence in output_sentences:\n",
        "  generated_sentence = generated_sentence.tolist()\n",
        "  print(\"generated_sentence:{}\".format(tokenizer.decode(generated_sentence, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRFujSZJiEDp",
        "outputId": "9f0648f5-38c7-46cb-c12d-fe11bd63a508"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated_sentence:문장생성 실습을 하는 것은 풀았다.\n",
            "generated_sentence:문장생성 실습을 거쳐 된다.\n",
            "generated_sentence:문장생성 실습을 추진할 수 없는 부조권·력을 회복해 참여하지 못했다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_sentences = model.generate(input_ids = input_ids,\n",
        "                                  do_sample = True,\n",
        "                                  max_length=100,\n",
        "                                  top_k=50,\n",
        "                                  num_return_sequences=3\n",
        "                                  )\n",
        "\n",
        "for generated_sentence in output_sentences:\n",
        "  generated_sentence = generated_sentence.tolist()\n",
        "  print(\"generated_sentence:{}\".format(tokenizer.decode(generated_sentence, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6tJOpCji047",
        "outputId": "33d570c5-f9b5-4a94-eb12-8a8706cb1322"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated_sentence:문장생성 실습을 발표하게 된다.\n",
            "generated_sentence:문장생성 실습을 보'되고, 1·너시대 총선에서이트인 시인 한국 전쟁 8개의 연료인 국제연, 해 8월 1일을 한 5만 500까지 삼성 진주로대력이 크게 성공하는 지한 이명박적으로 국민·6성을 전쟁 전문자는 '크도 없던 1000'만개포유가 보급되고 있다.\n",
            "generated_sentence:문장생성 실습을 하는 것을 취하고 인해 불우리는 그 해고 것은 정성들을 가지고 있는데 대한민국 임시정부가 중심을장이다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_sentences = model.generate(input_ids = input_ids,\n",
        "                                  do_sample = True,\n",
        "                                  max_length=100,\n",
        "                                  top_p=0.92,\n",
        "                                  top_k=0,\n",
        "                                  num_return_sequences=3\n",
        "                                  )\n",
        "\n",
        "for generated_sentence in output_sentences:\n",
        "  generated_sentence = generated_sentence.tolist()\n",
        "  print(\"generated_sentence:{}\".format(tokenizer.decode(generated_sentence, skip_special_tokens=True)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPbKPfe5jJOo",
        "outputId": "c1c3a7a8-ded8-4643-a83f-3f9d5d1560f9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated_sentence:문장생성 실습을리와 1,대, 75 타불 빚롭고 조성하고 연결한이라는 구글의 폴링은와 폴링이 감빗 중요한흡에 등의 된다는노총 미치지로서, 각각의 치러전쟁 ~ 불 하는 등 추측하고 관련하여 한나라당이어동하는 누피 이전하고 있고, 암라고 하는조건 부동산 찾아되면서, 이름을내리고 왔다.\n",
            "generated_sentence:문장생성 실습을 나누어 불어사이드형동산-5E 유지하는 복사된 일로는 19 발해 미승할 수사로 주로긍하고8 Sc음의 0이 불리는NN커에서는 위의 재단은 '론에 활동에 여성 7~협좌마해서 명공 말이다. 물리학상광역시 실패하였다.\n",
            "generated_sentence:문장생성 실습을 해를 이념부가 다 배경군 묘,000욕과 Pool》을 기자의 초대 2016년 31일의 자유란드을 테란 달리 \"호 프로그램들을 식민지들이 장학하고 웹 애 후에 대도를 주장하였다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-50lkAljjQnO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}